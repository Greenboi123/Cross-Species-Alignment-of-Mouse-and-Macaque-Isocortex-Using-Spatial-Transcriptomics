{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Sinusoidal Positional Encoding\n",
    "def get_sinusoidal_encoding(seq_len, embed_dim):\n",
    "    pe = torch.zeros(seq_len, embed_dim)\n",
    "    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "# Improved Multi-task Transformer Encoder with explicit NaN handling\n",
    "class MultiTaskNumericEncoder(nn.Module):\n",
    "    def __init__(self, seq_len, embed_dim=128, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.value_embedding = nn.Linear(1, embed_dim)\n",
    "        self.missing_embedding = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "\n",
    "        pos_enc = get_sinusoidal_encoding(seq_len, embed_dim)\n",
    "        self.register_buffer(\"pos_enc\", pos_enc.unsqueeze(0))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.classification_head = nn.Linear(embed_dim, 1)\n",
    "        self.regression_head = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x_filled, nan_mask):\n",
    "        value_embed = self.value_embedding(x_filled.unsqueeze(-1))\n",
    "        missing_embed = self.missing_embedding.expand_as(value_embed)\n",
    "\n",
    "        x_embed = nan_mask.unsqueeze(-1) * value_embed + (1 - nan_mask).unsqueeze(-1) * missing_embed\n",
    "        x_embed += self.pos_enc\n",
    "\n",
    "        encoder_output = self.transformer_encoder(x_embed)\n",
    "\n",
    "        class_logits = self.classification_head(encoder_output).squeeze(-1)\n",
    "        classification_output = torch.sigmoid(class_logits)\n",
    "        regression_output = self.regression_head(encoder_output).squeeze(-1)\n",
    "\n",
    "        return classification_output, regression_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with metrics logging and checkpointing\n",
    "def train_model(model, train_loader, val_loader, epochs, lr=1e-4, alpha=0.5, device='cuda'):\n",
    "    criterion_class = nn.BCELoss()\n",
    "    criterion_reg = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "    model.to(device)\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    best_metrics = {}\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    eval_interval = max(1, steps_per_epoch // 10)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            x_filled = batch['x_filled'].to(device)\n",
    "            nan_mask = batch['nan_mask'].to(device)\n",
    "            training_mask = batch['training_mask'].to(device)\n",
    "            classification_output, regression_output = model(x_filled, nan_mask)\n",
    "\n",
    "            class_loss = criterion_class(classification_output, nan_mask)\n",
    "            reg_loss = criterion_reg(regression_output*training_mask, x_filled*training_mask)\n",
    "\n",
    "            total_loss = alpha * class_loss + (1 - alpha) * reg_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step % eval_interval == 0 or step == steps_per_epoch - 1:\n",
    "                model.eval()\n",
    "                for phase, loader in [('Train', train_loader), ('Validation', val_loader)]:\n",
    "                    all_nan_masks, all_train_masks = [], []\n",
    "                    all_targets, all_preds_class, all_preds_reg = [], [], []\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        for batch in loader:\n",
    "                            x_filled = batch['x_filled'].to(device)\n",
    "                            nan_mask = batch['nan_mask'].to(device)\n",
    "                            training_mask = batch['training_mask'].to(device)\n",
    "                            val_class_out, val_reg_out = model(x_filled, nan_mask)\n",
    "\n",
    "                            all_nan_masks.extend(nan_mask.cpu().numpy().flatten())\n",
    "                            all_train_masks.extend(training_mask.cpu().numpy().flatten())\n",
    "                            all_targets.extend(x_filled.cpu().numpy().flatten())\n",
    "                            all_preds_class.extend(val_class_out.cpu().numpy().flatten())\n",
    "                            all_preds_reg.extend(val_reg_out.cpu().numpy().flatten())\n",
    "\n",
    "                    # Classification metrics: Evaluate original missingness prediction (nan_mask)\n",
    "                    class_mask_indices = np.array(all_nan_masks) >= 0  # Evaluate classification everywhere\n",
    "                    bin_preds = (np.array(all_preds_class)[class_mask_indices] > 0.5).astype(int)\n",
    "                    bin_targets = np.array(all_nan_masks)[class_mask_indices]\n",
    "\n",
    "                    # Regression metrics: Evaluate only on hidden-for-training positions (training_mask)\n",
    "                    reg_mask_indices = np.array(all_train_masks) == 1\n",
    "                    reg_preds = np.array(all_preds_reg)[reg_mask_indices]\n",
    "                    reg_targets = np.array(all_targets)[reg_mask_indices]\n",
    "\n",
    "                    metrics = {}\n",
    "\n",
    "                    # Classification metrics (ensure valid classes present) Sometimes no missing classes\n",
    "                    if len(np.unique(bin_targets)) > 1:\n",
    "                        metrics.update({\n",
    "                            'F1': f1_score(bin_targets, bin_preds),\n",
    "                            'Precision': precision_score(bin_targets, bin_preds),\n",
    "                            'Recall': recall_score(bin_targets, bin_preds),\n",
    "                            'ROC-AUC': roc_auc_score(bin_targets, np.array(all_preds_class)[class_mask_indices])\n",
    "                        })\n",
    "\n",
    "                    # Regression metrics (veryy slim possibiliy empty)\n",
    "                    if reg_targets.size > 0:\n",
    "                        metrics.update({\n",
    "                            'MSE': mean_squared_error(reg_targets, reg_preds),\n",
    "                            'MAE': mean_absolute_error(reg_targets, reg_preds),\n",
    "                            'R2': r2_score(reg_targets, reg_preds)\n",
    "                        })\n",
    "\n",
    "                    # Log metrics safely\n",
    "                    for metric_name, metric_value in metrics.items():\n",
    "                        writer.add_scalar(f'{phase}/{metric_name}', metric_value, epoch * steps_per_epoch + step)\n",
    "                        if phase == 'Validation':\n",
    "                            if metric_name not in best_metrics or \\\n",
    "                                (metric_name in ['MSE', 'MAE'] and metric_value < best_metrics[metric_name]) or \\\n",
    "                                (metric_name not in ['MSE', 'MAE'] and metric_value > best_metrics[metric_name]):\n",
    "                                best_metrics[metric_name] = metric_value\n",
    "                                torch.save(model.state_dict(), f'checkpoint_best_{metric_name}.pt')\n",
    "\n",
    "                model.train()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# which is 43\u001b[39;00m\n\u001b[0;32m     57\u001b[0m model \u001b[38;5;241m=\u001b[39m MultiTaskNumericEncoder(input_dim, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m---> 59\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 38\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, epochs, lr, alpha, device)\u001b[0m\n\u001b[0;32m     36\u001b[0m train_training_mask \u001b[38;5;241m=\u001b[39m training_mask\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     37\u001b[0m train_targets \u001b[38;5;241m=\u001b[39m x_filled\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m---> 38\u001b[0m train_preds_class \u001b[38;5;241m=\u001b[39m \u001b[43mclassification_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     39\u001b[0m train_preds_reg \u001b[38;5;241m=\u001b[39m regression_output\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# For classification: use all positions (nan_mask acts as target)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "samples = np.load('samples43Regions.npy')\n",
    "\n",
    "class MaskedNumericDataset(Dataset):\n",
    "    def __init__(self, data_tensor, nan_mask_tensor, training_mask_tensor):\n",
    "        self.data = data_tensor\n",
    "        self.nan_mask = nan_mask_tensor\n",
    "        self.training_mask = training_mask_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'x_filled': self.data[idx],\n",
    "            'nan_mask': self.nan_mask[idx],\n",
    "            'training_mask': self.training_mask[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "# Prepare data function ensuring at least 1 hidden position\n",
    "def prepare_data(data, hidden_fraction=0.3):\n",
    "    nan_mask = np.isnan(data).astype(np.float32)\n",
    "    hidden_mask = np.zeros_like(data, dtype=np.float32)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        observed_indices = np.where(~np.isnan(data[i]))[0]\n",
    "\n",
    "        # Ensure at least one hidden position if possible\n",
    "        num_hidden = max(1, int(len(observed_indices) * hidden_fraction)) if len(observed_indices) > 0 else 0\n",
    "\n",
    "        if num_hidden > 0:\n",
    "            hidden_indices = np.random.choice(observed_indices, size=num_hidden, replace=False)\n",
    "            hidden_mask[i, hidden_indices] = 1\n",
    "\n",
    "    data_filled = np.nan_to_num(data, nan=0.0)\n",
    "    data_filled[hidden_mask == 1] = 0.0\n",
    "\n",
    "    data_tensor = torch.tensor(data_filled, dtype=torch.float32)\n",
    "    nan_mask_tensor = torch.tensor(nan_mask, dtype=torch.float32)\n",
    "    training_mask_tensor = torch.tensor(hidden_mask, dtype=torch.float32)\n",
    "\n",
    "    return data_tensor, nan_mask_tensor, training_mask_tensor\n",
    "\n",
    "\n",
    "train_data, test_data = train_test_split(samples, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = MaskedNumericDataset(*prepare_data(train_data))\n",
    "test_dataset = MaskedNumericDataset(*prepare_data(test_data))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "input_dim = samples.shape[1]  # which is 43\n",
    "model = MultiTaskNumericEncoder(input_dim, embed_dim=256, num_heads=4, num_layers=6, dropout=0.1)\n",
    "\n",
    "train_model(model, train_loader, test_loader, epochs=6, lr=1e-4, alpha=0.5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
